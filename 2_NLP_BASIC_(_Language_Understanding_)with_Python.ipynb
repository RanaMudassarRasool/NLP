{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZJHL6jLSY+TJnwzem3sWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranamaddy/NLP/blob/main/2_NLP_BASIC_(_Language_Understanding_)with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Understanding**\n",
        "\n",
        "NLTK can be used for Language Understanding, which is a subfield of Natural Language Processing (NLP) that focuses on enabling computers to understand human language.\n",
        "\n",
        "NLTK provides a range of tools and resources for language understanding tasks such as part-of-speech tagging, named entity recognition, dependency parsing, and sentiment analysis.\n",
        "\n",
        "For example, NLTK's pos_tag() function can be used to tag the parts of speech in a sentence, which can be useful for understanding the grammatical structure of the text. NLTK's ne_chunk() function can be used for named entity recognition, which can help identify and extract entities such as people, organizations, and locations from the text. NLTK also provides pre-trained models for sentiment analysis, which can be used to classify the sentiment of a given text as positive, negative, or neutral.\n",
        "\n",
        "While NLTK is a powerful tool for language understanding, it may not always be the best option for more complex tasks or large-scale applications. However, it can be a good starting point for learning and experimenting with language understanding techniques in NLP.\n",
        "\n",
        "**NLTK can be used** for Language Understanding, which is a subfield of Natural Language Processing (NLP) that focuses on enabling computers to understand human language. NLTK provides a range of tools and resources for language understanding tasks such as part-of-speech tagging, named entity recognition, dependency parsing, and sentiment analysis.\n",
        "\n",
        "Here are some examples of how to use NLTK for Language Understanding:"
      ],
      "metadata": {
        "id": "ODfOFGvMlf3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-of-speech (POS)** tagging: The pos_tag() function in NLTK can be used to tag the parts of speech in a sentence. For example:"
      ],
      "metadata": {
        "id": "tBkLEN3Ql2a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first downloaded the required resource for POS tagging using **nltk.download()**. Then, we defined a simple text string, tokenized it using **word_tokenize()**, and tagged the parts of speech using pos_tag(). The output is a list of tuples, where each tuple contains a token and its corresponding POS tag."
      ],
      "metadata": {
        "id": "Kqx0k1vamMV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh6etTARlXcI",
        "outputId": "e5cc979c-0c1b-4217-d8b9-c3bea78b1d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP'), ('saw', 'VBD'), ('the', 'DT'), ('blue', 'JJ'), ('car', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"John saw the blue car.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition (NER):** The ne_chunk() function in NLTK can be used for named entity recognition, which can help identify and extract entities such as people, organizations, and locations from the text. For example:"
      ],
      "metadata": {
        "id": "K79bTlgumRFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first downloaded the required resources for named entity recognition using **nltk.download()**. Then, we defined a simple text string, tokenized it and tagged the parts of speech using**pos_tag()**. Finally, we applied **ne_chunk()** to the POS tags to extract named entities. The output is a tree structure, where each leaf node represents a token and its entity label."
      ],
      "metadata": {
        "id": "wC2aYbY6mmbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "ne_tags = nltk.ne_chunk(pos_tags)\n",
        "print(ne_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNMXUyswmcSv",
        "outputId": "8254431a-c72c-4378-9f3d-e6ecabf40510"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment analysis:** NLTK also provides pre-trained models for sentiment analysis, which can be used to classify the sentiment of a given text as positive, negative, or neutral. For example:"
      ],
      "metadata": {
        "id": "lNShLSvRmsgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we downloaded the required resource for sentiment analysis using **nltk.download().** Then, we defined a simple text string and used the **SentimentIntensityAnalyzer()**class to analyze its sentiment. The output is a dictionary of scores, where pos and neg indicate the proportions of positive and negative sentiment in the text, neu indicates the proportion of neutral sentiment, and compound provides an overall sentiment\n",
        "\n",
        " score between -1 (most negative) and 1 (most positive)."
      ],
      "metadata": {
        "id": "XUwHFSaFm4aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "text = \"I love this movie! It's so funny and entertaining.\"\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "scores = analyzer.polarity_scores(text)\n",
        "print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSrAhicumxX6",
        "outputId": "74edc500-0799-4ec4-dc67-e5a28b9f526e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.305, 'pos': 0.695, 'compound': 0.9081}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**===============================================================================================**"
      ],
      "metadata": {
        "id": "B73m4OjQnnUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Understanding BY spaCy**\n",
        "\n",
        "Python libraries such as spaCy provide advanced Language Understanding capabilities for Natural Language Processing (NLP) tasks. spaCy is an open-source library that provides a range of features for efficient and accurate language processing, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and semantic similarity.\n",
        "\n",
        "Here are some examples of how to use spaCy for Language Understandin\n"
      ],
      "metadata": {
        "id": "yquZ0yIHns-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization: spaCy's** tokenizer can be used to split text into individual tokens, which are the basic units of language processing. For example:"
      ],
      "metadata": {
        "id": "KzjZdBKKn8tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first loaded spaCy's pre-trained English language model using **spacy.load().** Then, we defined a simple text string and created a Doc object using the nlp() function. Finally, we looped through each token in the Doc object and printed its text using token.text."
      ],
      "metadata": {
        "id": "k8ga55ZloLqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KnHhh7nnrl6",
        "outputId": "c26c43c0-d28d-4cb5-b74f-817e82b83dcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jumped\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition (NER): spaCy's** EntityRecognizer can be used for named entity recognition, which can help identify and extract entities such as people, organizations, and locations from the text. For example:"
      ],
      "metadata": {
        "id": "wjCdLSsLoR2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first loaded spaCy's pre-trained English language model using **spacy.load().** Then, we defined a simple text string and created a Doc object using the nlp() function. Finally, we looped through each named entity in the Doc object and printed its text and entity label using ent.text and ent.label_"
      ],
      "metadata": {
        "id": "UN3tLRnHodLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwBpu0SVoV8j",
        "outputId": "ddc29b08-efca-44fc-f85e-6cad3ed8fda0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama PERSON\n",
            "Hawaii GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic similarity: spaCy** provides a similarity() method for comparing the similarity between two documents or spans based on their semantic meaning. For example"
      ],
      "metadata": {
        "id": "-DH1kMmholRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first loaded spaCy's pre-trained English language model using **spacy.load()**. Then, we defined two simple text strings and created Doc objects for each using the **nlp()** function. Finally, we used the **similarity()** method to compare the semantic similarity between the two documents, with the output being a similarity score between 0 (completely dissimilar) and 1 (identical)"
      ],
      "metadata": {
        "id": "pNslDcSNo0C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc1 = nlp(\"The cat sat on the mat.\")\n",
        "doc2 = nlp(\"The dog slept on the rug.\")\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP6jAe3KooXX",
        "outputId": "a6e19d15-1090-4084-923f-9c8f34b23afd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8641852424995727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fa45cf731201>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependency Parsing**: Dependency parsing is the process of analyzing the grammatical structure of a sentence by identifying the relationships between words. spaCy provides a DependencyParser that can be used to perform dependency parsing on a text. For example:"
      ],
      "metadata": {
        "id": "bu66TJS8pe8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first loaded spaCy's pre-trained English language model using **spacy.load()**. Then, we defined a simple text string and created a Doc object using the **nlp()** function. Finally, we looped through each token in the Doc object and printed its text, dependency label, head word, head word POS tag, and children using the **token.text, token.dep_, token.head.text, token.head.pos_, and token.children attributes**.\n",
        "\n",
        "This example shows how to extract the grammatical structure of a sentence using dependency parsing. The output includes the text of each token, its dependency label (e.g., subject, object, etc.), its head word (i.e., the token it depends on), its head word POS tag, and its children (i.e., the tokens that depend on it)."
      ],
      "metadata": {
        "id": "JWnUXRgIppgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"I ate pizza with a fork.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "          [child for child in token.children])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7YGAqrzpi68",
        "outputId": "2f949384-2d0d-47fc-936e-d17533d269eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I nsubj ate VERB []\n",
            "ate ROOT ate VERB [I, pizza, with, .]\n",
            "pizza dobj ate VERB []\n",
            "with prep ate VERB [fork]\n",
            "a det fork NOUN []\n",
            "fork pobj with ADP [a]\n",
            ". punct ate VERB []\n"
          ]
        }
      ]
    }
  ]
}