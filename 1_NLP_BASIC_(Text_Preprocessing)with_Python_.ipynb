{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWgCfkCy0rDo7Y7yx53Q8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranamaddy/NLP/blob/main/1_NLP_BASIC_(Text_Preprocessing)with_Python_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP outline for python**\n",
        "\n",
        "Here is an outline of Natural Language Processing (NLP) in Python:\n",
        "\n",
        "**Text Preprocessing**: Python provides several libraries for text preprocessing, including NLTK, spaCy, and TextBlob. These libraries can be used for tasks such as tokenization, stemming, and part-of-speech tagging.\n",
        "\n",
        "**Language Understanding**: Python libraries such as spaCy and TextBlob provide tools for language understanding, including named entity recognition and semantic analysis.\n",
        "\n",
        "**Language Generation**: Python libraries such as NLTK and TextBlob can be used for language generation tasks such as text summarization and machine translation.\n",
        "\n",
        "**Sentiment Analysis:** Python libraries such as TextBlob and NLTK provide tools for sentiment analysis, allowing you to determine the emotional tone or sentiment behind text.\n",
        "\n",
        "**Speech Processing:** Python provides libraries for speech processing, such as SpeechRecognition and pyttsx3, which can be used for tasks such as speech recognition and speech synthesis.\n",
        "\n",
        "**Machine Learning**: Python provides several libraries for machine learning, such as scikit-learn and TensorFlow, which can be used for tasks such as text classification and language modeling.\n",
        "\n",
        "**Web Scraping:** Python's BeautifulSoup library can be used for web scraping, allowing you to extract text data from websites for NLP tasks.\n",
        "\n",
        "Python is a popular language for NLP due to its ease of use and the availability of numerous powerful libraries. By using these libraries and tools, you can perform a wide range of NLP tasks in Python, from simple text preprocessing to complex machine learning model"
      ],
      "metadata": {
        "id": "shvReklehIh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing By NLTK**\n",
        "\n",
        "Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and preparing text data for further analysis. The NLTK (Natural Language Toolkit) library in Python provides several tools for text preprocessing, including:\n",
        "\n",
        "1. Tokenization: Tokenization is the process of splitting text into individual words or tokens. NLTK provides several tokenizers, including the word tokenizer, which splits text into individual words, and the sentence tokenizer, which splits text into individual sentences.\n",
        "Example:\n"
      ],
      "metadata": {
        "id": "7scepI0yhxB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM49WUbIhCce",
        "outputId": "19874c1d-c8cb-41b8-ae1c-419bb24fe112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', '.', 'It', 'contains', 'punctuation', 'marks', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is a sample sentence. It contains punctuation marks!\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Stopword Removal:** Stopwords are commonly used words that do not carry significant meaning, such as \"the\" and \"and\". NLTK provides a list of stopwords that can be removed from text."
      ],
      "metadata": {
        "id": "q04b0YutiLe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = [\"I\", \"am\", \"not\", \"interested\", \"in\", \"this\", \"topic\", \".\", \"It\", \"is\", \"boring\", \".\"]\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koHMbcluiP3k",
        "outputId": "d4b8cfe6-fde0-4842-e825-9b7b0743a357"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['interested', 'topic', '.', 'boring', '.']\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Stemming and Lemmatization:** Stemming and lemmatization are techniques used to reduce words to their base form. NLTK provides several stemmers and lemmatizers, including the Porter stemmer and the WordNet lemmatizer."
      ],
      "metadata": {
        "id": "CqLp4urCityN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"He was running and eating at the same time. He has a bad habit of running in the street.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(stemmed_tokens)\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcgqQnzDixfd",
        "outputId": "5c7da2b1-be1f-4239-84ad-765a8fd94143"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'wa', 'run', 'and', 'eat', 'at', 'the', 'same', 'time', '.', 'he', 'ha', 'a', 'bad', 'habit', 'of', 'run', 'in', 'the', 'street', '.']\n",
            "['He', 'wa', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', '.', 'He', 'ha', 'a', 'bad', 'habit', 'of', 'running', 'in', 'the', 'street', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Part-of-Speech Tagging:**\n",
        "\n",
        " Part-of-speech (POS) tagging involves labeling each word in a text with its corresponding part of speech, such as noun, verb, or adjective. NLTK provides a pre-trained POS tagger that can be used to tag words in a text."
      ],
      "metadata": {
        "id": "nlpxUZTTjO6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"John likes to play football in the park.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "print(tagged_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBcCOO0GjUG0",
        "outputId": "92d572a5-0f88-4173-eaae-32dd3a1769a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Named Entity Recognition:**\n",
        " Named entity recognition (NER) involves identifying and labeling named entities in a text, such as names, organizations, and locations. NLTK provides a pre-trained NER tagger that can be used to identify named entities in a text."
      ],
      "metadata": {
        "id": "R6jkMd0DjgkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "text = \"Barack Obama was the 44th President of the United States of America.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "named_entities = ne_chunk(tagged_tokens)\n",
        "\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua2lLDrkjlzW",
        "outputId": "3fa12752-73cf-44f7-f1dd-e08a2e48f678"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  of/IN\n",
            "  (GPE America/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Word Embeddings**: Word embeddings are dense vector representations of words that capture their semantic meaning. NLTK provides several pre-trained word embeddings, such as Word2Vec and GloVe, that can be used to represent words in a text as vectors."
      ],
      "metadata": {
        "id": "d3t5uwTTjtKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"I like to eat pizza and pasta. Pizza is my favorite food.\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "model = Word2Vec(tokenized_sentences, min_count=1)\n",
        "\n",
        "pizza_vector = model.wv['pizza']\n",
        "print(pizza_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajGX2JijsSL",
        "outputId": "881e6813-5b8f-436e-e2e9-0c0864aeb93d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-9.5766364e-03  8.9430977e-03  4.1696127e-03  9.2342924e-03\n",
            "  6.6383476e-03  2.9204944e-03  9.8048961e-03 -4.4205645e-03\n",
            " -6.8064686e-03  4.2235320e-03  3.7343241e-03 -5.6667118e-03\n",
            "  9.7071612e-03 -3.5584576e-03  9.5499996e-03  8.3605840e-04\n",
            " -6.3371290e-03 -1.9779752e-03 -7.3787277e-03 -2.9853259e-03\n",
            "  1.0427843e-03  9.4836736e-03  9.3608815e-03 -6.5972861e-03\n",
            "  3.4747063e-03  2.2763265e-03 -2.4899244e-03 -9.2293778e-03\n",
            "  1.0275932e-03 -8.1658373e-03  6.3170199e-03 -5.8003748e-03\n",
            "  5.5417293e-03  9.8350449e-03 -1.5834211e-04  4.5298999e-03\n",
            " -1.8063794e-03  7.3588388e-03  3.9380039e-03 -9.0122502e-03\n",
            " -2.4018812e-03  3.6301538e-03 -1.0429607e-04 -1.1998524e-03\n",
            " -1.0579068e-03 -1.6744122e-03  6.0184632e-04  4.1674497e-03\n",
            " -4.2544161e-03 -3.8310036e-03 -5.7033147e-05  2.7015869e-04\n",
            " -1.7265114e-04 -4.7848234e-03  4.3171979e-03 -2.1732899e-03\n",
            "  2.1029287e-03  6.6403928e-04  5.9715942e-03 -6.8387738e-03\n",
            " -6.8197562e-03 -4.4776555e-03  9.4360467e-03 -1.5933317e-03\n",
            " -9.4336243e-03 -5.4271799e-04 -4.4525662e-03  5.9992573e-03\n",
            " -9.5837042e-03  2.8585226e-03 -9.2505179e-03  1.2554345e-03\n",
            "  5.9978184e-03  7.3985797e-03 -7.6200380e-03 -6.0496074e-03\n",
            " -6.8346621e-03 -7.9223309e-03 -9.5015457e-03 -2.1256735e-03\n",
            " -8.3417597e-04 -7.2526387e-03  6.7914347e-03  1.1245741e-03\n",
            "  5.8240173e-03  1.4750307e-03  7.8927341e-04 -7.3685893e-03\n",
            " -2.1754366e-03  4.3248092e-03 -5.0824010e-03  1.1314496e-03\n",
            "  2.8840285e-03 -1.5342829e-03  9.9334633e-03  8.3515141e-03\n",
            "  2.4167579e-03  7.1146749e-03  5.8933995e-03 -5.5820951e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define text to preprocess\n",
        "text = \"The quick brown foxes jumped over the lazy dog's tail. The dogs didn't seem to mind.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
        "\n",
        "# Lemmatize the remaining tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "# Print the result\n",
        "print(lemmatized_tokens)\n",
        "#The code above preprocesses a given text by converting it to lowercase, tokenizing it,\n",
        "# removing stopwords, and lemmatizing the remaining tokens. The resulting lemmatized tokens are then printed."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgUTl3XekVHP",
        "outputId": "1bd4c561-479f-4543-b913-3ae199c05031"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', \"'s\", 'tail', '.', 'dog', \"n't\", 'seem', 'mind', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}